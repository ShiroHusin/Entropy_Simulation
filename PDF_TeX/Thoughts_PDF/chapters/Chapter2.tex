\chapter{Stochastic processes}
\section{Extending the idea}
 In section 1.2.3, I introduced the idea that you could actually have 2 sequences $S_{1}$ and $S_{2}$ with a set number of 1s in each. Now I would like to extend the idea further and have a sequence $S_{k}=[1,0,0,1,0,0,1,0,0....]$ of length $l$, $l$ times. This means the final system will be:
$$M=\begin{bmatrix}
S_{1} \\
S_{2}\\
S_{3}\\
... \\
S_{l}
\end{bmatrix}$$
What we end up with is a square matrix M with $l^{2}$ elements. Each of these elements will still be discrete with it only taking binary values of 1 or 0.\par

\vspace{0.3cm}
 To model real-life phenomena of heat transfer, conservation of energy, and conduction. I would introduce 3 things these are:
\begin{itemize}
    \item \textbf{Law 1: }A cell can either be a 1 or a 0, on or off.
    \item \textbf{Law 2: }The number of 1s ($E$) within the Matrix M cannot change.
    \item \textbf{Law 3: }Each of the 1s can move to an empty square (0) that is 1 square surrounding it.
    \item \textbf{Features: }There is a probability multiplier labelled $\alpha$ which modifies the chances of movement. If $\alpha=0$ nothing will happen. If $\alpha=10\%$, the arrows at \ref{fig:directions_1} have a 1.25\% chance of happening each. If $\alpha=100\%$, the arrows are 12.5\% each. Generally, this means that if it can move it will move if it can't it won't. 
\end{itemize}
We can then enclose these 3 bullet points within a function $F(M, \alpha)$. For k iterations, the output is $M_{k+1}=F(M_{k}, \alpha)$. This is related to Figure \ref{fig:my_label} where the output at a specific iteration is fed back as the input for the next iteration until a certain number of iterations is reached. The second rule is an analog of the 1st law of thermodynamics. The third law governs heat transfer and the feature mirrors that of heat conduction. To provide a clearer way of how the third law works consider the following Figure \ref{fig:valid_directions}. 
\begin{figure}[H]
     \centering
     \begin{subfigure}[h]{0.3\textwidth}
         \centering
         \includegraphics[width=4cm,height=4cm]{images/valid1.png}
         \caption{Valid directions for a 1 surrounded by 0}
         \label{fig:directions_1}
     \end{subfigure}
      \hspace{0.7cm}
     \begin{subfigure}[h]{0.3\textwidth}
         \centering
         \includegraphics[width=4cm,height=4cm]{images/valid2.png}
         \caption{Valid directions for multiple 1s.}
         \label{fig:directions_2}
     \end{subfigure}
        \caption{An explanation of the 2nd law}
        \label{fig:valid_directions}
\end{figure}
The arrows depict the possible directions that an occupied cell (1) can go. Under appropriate code implementation, the 1s should not be able to jump 2 spaces apart. In the special situation where an occupied cell is completely surrounded by occupied cells, the only possible outcome is for it to remain in the same position. \par
\subsection{Computing entropy}
The next task is to find a way to compute the entropy of the matrix $M_{k}$ against its associated iteration. Allowing for k to be 600 means that the final entropy plot will contain 600 pairs of $(k, H(M_{k}))$ where $H(M)$ is the entropy function. We can compute the entropy in 2 ways: 
\begin{itemize}
    \item Adopt a modified version of \ref{Eq:Combination} and find integer combinations of $n_{1}+n_{2}+n_{3}+...+n_{l}=E$ and find their unique micro-state tally. 
    \item Adopt an imperfect approximation using Shannon entropy.
\end{itemize}
 Method 1 works for small numbers of $l$ but if $l \ge 50$. The computation of method 1 gets extremely tedious and computationally demanding. Likewise, I am too lazy to even think about the possible micro-states possible associated with a unique integer combination of $n_{1}+n_{2}+n_{3}+...+n_{l}=E$. Therefore, I will adopt method 2 within my simulation. \par
 
\vspace{0,3cm}
The Shannon entropy of a system is given as follows: 
\begin{equation}
    H=-\sum_{i=1}^{N} (P_{i})log_2(P_{i})
    \label{Eq:Shannon}
\end{equation}
However, I prefer the alternative version of Shannon entropy \href{https://www.youtube.com/watch?v=YtebGVx-Fxw}{explained} by Josh Starmer which is: 
\begin{equation}
    H=\sum_{i=1}^{N} (P_{i})log_{2}(\frac{1}{P_{i}})
    \label{Eq: Shannon_alternative}
\end{equation}

In plain English, Equation \ref{Eq: Shannon_alternative} says that for the associated probability, multiply it by its \emph{surprise} and sum it up for all the probabilities. \par

\vspace{0.3cm}
To compute the entropy for each of the iterations of $M_{k}$. I would rearrange matrix M and flatten it to a 1-dimensional sequence. This sequence $G$ can be written as:
$$G=[S_{1}, S_{2}...S_{l}]$$
The sequence G will almost look like a tape of 1s and 0s with length $l^{2}$. The next step is to partition the tape into 2-bit binary digits. This means that there will be $\frac{l^{2}}{2}$ partitions of 2-bit binary digits if $l^{2}$ were even. If $l^{2}$ is an odd number, the number of partitions of 2-bit binary digits is $ \floor{\frac{l^{2}}{2}}-1$. Now, that the partitioning is done, we can then calculate the Shannon entropy for each of the 2-bit binary partitions. The computation of this simple algorithm is demonstrated in the table down below: \par
\vspace{0.3cm}
While we can stop here, a specific situation could arise where if $l^{2}$ were an odd number and the digit at the last index of $G$ is a 1, the algorithm I described would miss it entirely. Hence, I modified it as shown in \ref{fig:XOR}.
\begin{center}
\begin{tabular}{ |p{3cm}||p{3cm}||p{3cm}|  }
\hline
Parameter& 1s & 0s \\
 \hline
 Probability  & $\frac{E}{2}$ & $\frac{2-E}{2}$\\  
 Surprise &   $\log_{2}\frac{2}{E}$ & $\log_{2}\frac{2}{2-E}$ \\[1.5ex] 
  \hline
\end{tabular}
\end{center}
Otherwise, note that E is a function that just calculates how many 1s are within a defined binary string of length $l$. The possible sequences of a 2-bit binary string are $(0,0), (1,0), (0,1), (1,1)$. For the situation of $(0,0)$ the E within the above table is 0. This means that the Shannon entropy for the $(0,0)$ string is: 
$$H((0,0))=\frac{0}{2}\log_{2}\frac{2}{0}+\frac{2}{2}\log_{2}\frac{2}{2}$$
The first term is undefined while the 2nd term yields a 0. Getting an undefined result is OK within the context of Shannon entropy as the surprise of an event that never happens can be considered to be 0. Hence for the string $(0,0)$ the Entropy associated with it is 0. This is also the case for the string $(1,1)$ where the entropy also adds up to 0. \par

\vspace{0.3cm}
For the situation of the strings $(1,0)$ and $(0,1)$ the entropy calculation yields:
$$H((1,0))=\frac{1}{2}\log_{2}\frac{2}{1}+\frac{1}{2}\log_{2}\frac{2}{1}\longrightarrow 1$$
The results for computing 4 different permutations of 2-bit binary strings are shown in the table below: 
\begin{center}
\begin{tabular}{ |p{3cm}||p{3cm}||p{3cm}|  }
\hline
Bit& 1 & 0 \\
 \hline
 1  & \textbf{False} & \textbf{True}\\  
 0 &  \textbf{True} & \textbf{False}\\
  \hline
\end{tabular}
\end{center}
 False (0) values are the result of having $(1,1), (0,0)$ pairs, and True (1) is returned when the pairs have different elements such as $(1,0), (0,1)$. This is the \textbf{XOR} gate. \par

\pagebreak
 The XOR gate has been used in determining the entropy for randomly generated binary strings. For instance, the work by Grenville J. Croll \cite{Grenville_J_Croll:2013} analyzed the entropy of a finite binary string. His algorithm is to compute the entropy of the original string, store that value and then take the XOR value of the original binary string of length $l$ 2-bits at a time. The resulting 1st \emph{derivative} of the string is a string of length $l-1$. We can then compute the entropy of the 1st derivative and then store this value. Next, we apply the XOR function again to the first derivative resulting in the 2nd \emph{derivative} and take its entropy. This algorithm stops until its last derivative is all 0s or 1s. Based on this information, we can determine whether the original string contains periodic patterns. Finally, after we computed the entropy for each derivative we multiplied it with a weight $w$ and sum them up to find the entropy of the original binary string. \par

 \vspace{0.3cm}
 While the above algorithm will give an accurate measurement of the entropy of a binary string of a flattened matrix $M$ or $G$, it would be computationally demanding to implement it alongside the original simulation. This is especially true for large square matrices such as 200x200 ($l=200$) with 40000 elements in matrix $G$. Hence for the simulation, I decided to opt for just applying the XOR function once which will result in a $l^2-1$ bit binary string. \par

\vspace{0.3cm}
Once the $l^2-1$ bit binary string is computed, the next step is to just sum up all the 1s of this string which reveals the number of unique occurrences of pairs of $(1,0), (0,1)$ within the original string. Depicting how this works is shown within Figure \ref{fig:XOR} down below: 
\begin{figure}[H]
    \centering
    \includegraphics[width=10cm,height=6cm]{images/XOR.png}
    \caption{The XOR machine moves 1 square right per iteration.}
    \label{fig:XOR}
\end{figure}
Using the algorithm depicted above, it is possible to determine the upper bound sum of the number of 1s of XOR(G). Noting that I defined $E$ as the number of 1s in matrix $M$ and that E is a constant. The upper bound and lower bounds of this proxy entropy $(H)$ are: 
$$0 \le H \le 2E$$
The only way $H$ can actually be equal to $2E$ is if only if the sequence $G$ is in the form of $G=[0, 1, 0, 1, 0, 1 .... 1, 0]$. A fully alternating binary string with the first and last index positions being a 0. In this scenario, the function XOR(G) will count unique occurrences of $(1,0), (0,1)$ twice and will double count the number of 1s in $G$. \par

\vspace{0.3cm}
The method that I described above can be criticized as being rather \emph{caveman} like in its way of thinking and some major flaws are: 
\begin{itemize}
    \item The algorithm fails when $E \ge 0.5 l^{2}$.
    \item $G=[0, 1, 0, 1, 0, 1 .... 1, 0]$ is not exactly disordered or entropic.
\end{itemize}
However, for a large square Matrix $M$ where $l=200$. I would want to limit $E$ to be $E \leq 0.05l^{2}$ which in this case, applies the game rules to matrix M such that $M_{k+1}=F(M_{k}, \alpha)$ the underlying grid will be more disordered if there are more unique occurrences of $(1,0), (0,1)$. \par

\vspace{0.3cm}
The code for implementing this monster is posted on my GitHub page with its specific repository at \href{https://github.com/ShiroHusin/Entropy_Simulation?fbclid=IwAR32gsNSPyy80Pt0_VytukZG9eIPFMg20cl77TeuiW-TH2A0giijnKegKTo}{Here}. The source code is named \href{https://github.com/ShiroHusin/Entropy_Simulation/blob/main/Automata.py}{Automata.py}
